{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NZu3LuJt-YB"
      },
      "source": [
        "# **Post-Hoc Concept Bottleneck Models Replication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsACqbz_Mvt1"
      },
      "source": [
        "This file contains essentially all the scripts necessary to replicate the results we obtained for reproducing the following claims:\n",
        "1. *PCBMs achieve comparable performance to the original model.*\n",
        "2. *PCBMs do not require labelled concept datasets.*\n",
        "3. *PCBMs allow for global model editing.*\n",
        "\n",
        "In addition, the extension experiments evaluating the performance of PCBMs on audio data can also be performed here. The experiments related to the user study are present [in another notebook](user_study.ipynb).\n",
        "\n",
        "This file assumes that the main README instructions have already been followed, which would be every step before the environment has been activated. If not, then you can view it [here](../README.md). Note that the instructions after the environment activation parallel those present here, meaning that it is possible to follow either for guidance (though here all that is generally needed is to run the cells, with maybe some command editing if desired and file setup).\n",
        "\n",
        "Before starting, all the necessary files need to first be prepared. This notebook, when run, will setup all the necessary installations in the environment. We need to first move outside of the `\\notebook` directory via the code block below. It should automatically setup the directory depending on whether this notebook is being run locally or on Google Colab.\n",
        "\n",
        "**Extra Note:** In many cells, the following part (`!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1`) exists due to the importing of AudioCLIP in the files which have this snippet in front of them. Currently, the only way to not have to use this is by downloading the AudioCLIP dependencies. This is due to how the repository is setup, with the snippet being a workaround due to time constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "REBF9Gh8t-YC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "b99c4209-ab77-4288-a274-1722c66c2ff4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret gh_pat does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-88570a779e58>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrepo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'actuarial'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://{userdata.get('gh_pat')}@github.com/{repo_name}.git\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone {url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install git+https://github.com/openai/CLIP.git # for the CLIP library'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret gh_pat does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    repo_name = 'actuarial'\n",
        "    url = f\"https://{userdata.get('gh_pat')}@github.com/{repo_name}.git\"\n",
        "    !git clone {url}\n",
        "    !pip install git+https://github.com/openai/CLIP.git # for the CLIP library\n",
        "    print(\"\\nCurrent Directory:\")\n",
        "    %cd 'put repo name here'\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "else: # automatically checks if the current directory is 'repo name'\n",
        "    curdir = Path.cwd()\n",
        "    print(\"Current Directory\", curdir)\n",
        "    repo_name = \"put repo name here\"\n",
        "    assert curdir.name == repo_name or curdir.parent.name == repo_name, \"Notebook cwd has to be on the project root\"\n",
        "    if curdir.name == \"notebooks\":\n",
        "        %cd ..\n",
        "        print(\"New Current Directory:\", curdir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git # for the CLIP library\n",
        "    # print(\"\\nCurrent Directory:\")\n",
        "    # %cd 'put repo name here'\n"
      ],
      "metadata": {
        "id": "7_lRzDJRPMQY",
        "outputId": "8ecb3342-1b41-46ae-9c89-e52d9d3a7c50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-oyzgh3nz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-oyzgh3nz\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=98b98e6a66bfee243f2f478bbb7e4077f73895180c241216db858411b41e68e5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0a4ph5th/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "kVVtxGKsPRuA",
        "outputId": "b83e5f88-f46d-48f7-cf38-44f8d2ce0ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEKzAYinMvt2"
      },
      "source": [
        "You should now be in the **Anonymous** main folder. This is important for running the files to ensure that they save/search in the correct locations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDLfg_dpMvt2"
      },
      "source": [
        "# **Downloading the Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h5LsR59Mvt2"
      },
      "source": [
        "## BRODEN Concepts Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EIcCrF9Mvt3"
      },
      "source": [
        "_Note_: There is a potential permission error which may arise when trying to download the files via this notebook. Manual downloading may be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHOz6uLCt-YF"
      },
      "outputs": [],
      "source": [
        "# Get the BRODEN concepts dataset\n",
        "!bash ./scripts/download_broden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D4VV_oVMvt3"
      },
      "source": [
        "## COCO-Stuff Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpkSCMTaMvt3"
      },
      "source": [
        "*Note:* The dataset is around 20 GB in total. Ensure you have enough space on your device before attempting to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEm-yldNt-YF"
      },
      "outputs": [],
      "source": [
        "# Get the COCO-stuff dataset (bash is needed to run the command below)\n",
        "!bash ./scripts/download_cocostuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwR69zhIMvt3"
      },
      "source": [
        "## CUB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiMcEyZ2Mvt3"
      },
      "outputs": [],
      "source": [
        "# Get the CUB dataset (bash is needed here to run the command below)\n",
        "!bash ./scripts/download_cub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DibXNh0Mvt4"
      },
      "source": [
        "## Derm7pt Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf4ueULMMvt4"
      },
      "source": [
        "_Note:_ Due to how the download has been setup by the Original Derm7pt authors, registering to their site is necessary to access the dataset, thus meaning that we need to perform some manual processing. As such, please perform the following steps:\n",
        "\n",
        "1. Go to the Derm7pt site [here](https://derm.cs.sfu.ca/Download.html).\n",
        "2. Fill in the form with the necessary details.\n",
        "3. The email received should contain the download link alongside the needed login credentials below it. Click the link and then fill in the details in the prompt given, which should automatically trigger the download afterwards.\n",
        "4. Extract the .zip file and rename the folder extracted to `derm7pt`.\n",
        "5. Move this folder to `./Anonymous/artifacts/data`.\n",
        "\n",
        "_Note:_ If desired, for Google Colab you can upload the dataset to Google Drive and copy it to the current session using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDp30yu_Mvt4"
      },
      "outputs": [],
      "source": [
        "# set the location variable to the specific directory in your Google Drive\n",
        "location = \"path/to/your/directory/in/drive\"\n",
        "\n",
        "# construct the source and destination paths\n",
        "source_path = f\"/content/drive/MyDrive/{location}\"\n",
        "destination_path = \"/content/Anonymous/artifacts/data\"\n",
        "\n",
        "# copy to the destination path\n",
        "!cp -r \"{source_path}\" \"{destination_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3o7ktY2Mvt4"
      },
      "source": [
        "## HAM10000 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJK92wFgMvt4"
      },
      "source": [
        "_Note:_ The HAM10000 Dataset is made available as a public Kaggle dataset. In order to download it through this script, make sure you have a Kaggle API token ready and place it (a .JSON file) in the following directory: `C:\\Users\\\\*your username*\\\\.kaggle`.\n",
        "\n",
        "To create a Kaggle API token, please do the following steps:\n",
        "\n",
        "1. Go to your [account settings](https://www.kaggle.com/account). You will need to create a Kaggle account if you do not have one already.\n",
        "2. Click on your profile icon > \"Settings\" > Scroll down to \"API\" > click \"Create New Token\"\n",
        "3. This will download a file named `kaggle.json`. Again remember to move it to the scripts folder in the **Anonymous** directory.\n",
        "\n",
        "If on Google Colab, upload your generated API token (`kaggle.json`) to any folder you want and paste the directory to that file to the `folder_containing_api` variable below.\n",
        "\n",
        "Afterwards, just run the following codeblock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZZLneuRMvt4"
      },
      "outputs": [],
      "source": [
        "# Get the HAM10K dataset (bash is needed here to run the command below)\n",
        "\n",
        "folder_containing_api = \"\" # Add your folder here if on Colab\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = f'/content/drive/MyDrive/{folder_containing_api}'\n",
        "\n",
        "!bash ./scripts/download_ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thHiXXFyMvt4"
      },
      "source": [
        "## SIIM-ISIC Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9f-0IpRMvt4"
      },
      "source": [
        "_Note_: The original dataset is around 23 GB in total. The version downloaded by this script is a trimmed-down version which replicates what the original authors did (totalling less than 2 GB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SX71T4uMvt4"
      },
      "outputs": [],
      "source": [
        "# Get the SIIM-ISIC dataset\n",
        "!bash ./scripts/download_siim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JahT-uQPMvt4"
      },
      "source": [
        "## Metashift Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8ASzosLMvt5"
      },
      "outputs": [],
      "source": [
        "# Get the Metashift dataset\n",
        "!bash ./scripts/download_metashift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh_eyGwuMvt5"
      },
      "source": [
        "...and that would be every dataset needed for reproducing the main results! If you'd like, you can also download the datasets and dependencies for the extension experiments (which totals ~7 GB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU2o_oXRMvt5"
      },
      "source": [
        "## ESC-50 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn8_d1CCMvt5"
      },
      "outputs": [],
      "source": [
        "# Get the ESC-50 Dataset\n",
        "!bash ./scripts/download_esc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ECFMUrDMvt5"
      },
      "source": [
        "## UrbanSound8K Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFVJVzuLMvt5"
      },
      "source": [
        "Similar to the HAM10000 Dataset, you need to have an API token ready. Follow the instructions [there](#ham10000-dataset) if you don't have one ready and would like guidance on how to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFKqj9HRMvt5"
      },
      "outputs": [],
      "source": [
        "# Get the UrbanSound8K Dataset\n",
        "\n",
        "folder_containing_api = \"\" # Add your folder here if on Colab\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = f'/content/drive/MyDrive/{folder_containing_api}'\n",
        "\n",
        "!bash ./scripts/download_us8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7LbpncMvt5"
      },
      "source": [
        "## AudioSet Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtdraPoMvt5"
      },
      "source": [
        "Similar to the HAM10000 Dataset, you need to have an API token ready. Follow the instructions [there](#ham10000-dataset) if you don't have one ready and would like guidance on how to do so.\n",
        "\n",
        "If the above has been done, the AudioSet script can then download the validation data, which is what we use for our experiments due to size constraints. Downloading the other data splits (such as the balanced training one one) requires `ffmpeg` installed to be fully installed on your device. To install `ffmpeg`, you can follow the instructions below:\n",
        "\n",
        "1. Go [here](https://github.com/yt-dlp/FFmpeg-Builds?tab=readme-ov-file#patches-applied) and download the file corresponding to your device (link is present in the `README`).\n",
        "2. Follow the instructions listed [here](https://www.hostinger.com/tutorials/how-to-install-ffmpeg). You can ignore/adjust all the steps related to downloading the file.\n",
        "3. Restart your device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5KRpg24Mvt5"
      },
      "outputs": [],
      "source": [
        "# Get the ESC-50 Dataset\n",
        "!bash ./scripts/download_audioset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUTCIhGSMvt6"
      },
      "source": [
        "The audio files will be downloaded the first time you use the `get_dataset` method. Afterwards, the data can be loaded directly without any further downloading.\n",
        "\n",
        "_Note_: The process for downloading additional audio files (for the balanced train split) will take *very* long (~13 hours or more)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZNNVf23Mvt6"
      },
      "source": [
        "## AudioCLIP Dependencies\n",
        "\n",
        "**Please Note:** Due to how everything is setup, running the below script is necessary to run the experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoO-Zpk6Mvt6"
      },
      "source": [
        "This downloader only includes the fully pretrained AudioCLIP model and the vocabulary in case needed. The reason being that the main repository for AudioCLIP is not designed to be installed as a Python package. As of writing, no `setup.py` files or anything that would work has been implemented, making it not possible to directly install their repo.\n",
        "\n",
        "Thus, a copy of it has been integrated here, with the assets separated to prevent bottlenecking this repo. You can find the citation to the original authors [here](../models/AudioCLIP/README.md) and their original repository [here](https://github.com/AndreyGuzhov/AudioCLIP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFVfjSmEMvt6"
      },
      "outputs": [],
      "source": [
        "# Get the AudioCLIP Dependencies\n",
        "!bash ./scripts/download_audioclip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXDjzRufMvt6"
      },
      "source": [
        "# **Training and Evaluating PCBMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7JChpKYMvt6"
      },
      "source": [
        "Now that we have prepared all the necessary files, we can now begin with replicating the results obtained.\n",
        "\n",
        "Do note however, that some details for replication are missing, meaning that the results may somewhat differ compared to the original paper (which is also influenced by the hardware differences between experiments).\n",
        "\n",
        "As a reference for how you should specify the `dataset-name` and `backbone-name`, we provide the following table:\n",
        "\n",
        "### **Datasets**\n",
        "| Dataset | Codename for Parameter |\n",
        "| :-: | :-: |\n",
        "| CIFAR-10 | 'cifar10' |\n",
        "| CIFAR-100 | 'cifar100' |\n",
        "| COCO-Stuff | 'coco-stuff' |\n",
        "| CUB | 'cub' |\n",
        "| HAM10000 | 'ham10000' |\n",
        "| SIIM-ISIC | 'siim-isic' |\n",
        "| AudioSet | 'audioset' |\n",
        "| ESC-50 | 'esc50' |\n",
        "| UrbanSound8k | 'us8k' |\n",
        "| AudioSet | 'audioset' |\n",
        "\n",
        "### **Backbone Models**\n",
        "| Backbone | Codename for Parameter |\n",
        "| :-: | :-: |\n",
        "| ResNet18 | 'resnet18_cub' |\n",
        "| CLIP | 'clip:RN50' |\n",
        "| Inception | 'ham10000_inception' |\n",
        "| ImageNet ResNet18 | 'resnet18_imagenet1k_v1' |\n",
        "| AudioCLIP | 'audio' |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc0YWOrmMvt7"
      },
      "source": [
        "# Learning Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pTFLcURMvt7"
      },
      "source": [
        "In total, there are three concepts datasets needed for these experiments:\n",
        "1. BRODEN\n",
        "2. CUB\n",
        "3. Derm7pt\n",
        "\n",
        "Here we prepare each of these concepts for later use alongside the corresponding models, starting with the BRODEN ones.\n",
        "\n",
        "_Note:_ If you are on Colab, make sure to install PyTorch Ignite and Visdom first.\n",
        "\n",
        "**Extra Note:** If the concept bank was generated using CLIP, it will be saved without the colon (`:`) in the filename due to it causing the filename to be split. Keep this in mind when specifying the concept bank path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d33TtRljMvt7"
      },
      "outputs": [],
      "source": [
        "# To install PyTorch Ignite and Visdom\n",
        "if IN_COLAB:\n",
        "    !pip install pytorch-ignite\n",
        "    !pip install visdom\n",
        "    !pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_64QmNQMvt7"
      },
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"broden\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBxBPB3cMvt7"
      },
      "source": [
        "Then we move on to the CUB concepts,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3J9SFChMvt8"
      },
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"cub\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfEDYwEkMvt8"
      },
      "source": [
        "... and finally the Derm7pt concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1mjQHw5Mvt8"
      },
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"derm7pt\" \\\n",
        "  --backbone-name=\"ham10000_inception\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxJA0zsjMvt8"
      },
      "source": [
        "## Learning Multimodal Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_SWsD5GMvt8"
      },
      "source": [
        "One part of the original paper involves learning concepts automatically by utilizing CLIP embeddings. This has already been implemented by the authors in `learn_multimodal_concepts.py` (though some adjustments for improved readability and extra concept banks for extension studies have been made by us).\n",
        "\n",
        "Because of this, we can run the following snippets directly for each dataset, starting with CIFAR10/100 below.\n",
        "\n",
        "_Note:_ Make sure to change the device to match what you would like/have (by default it assumes _cuda_). Also, the `recurse` parameter defines how much we recurse through ConceptNet, meaning that a higher recurse value results in more (but less directly related) concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikj3vPUdMvt8"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"cifar10\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V0nAO6HMvt8"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"cifar100\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\" \\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHgf1Wv5Mvt8"
      },
      "source": [
        "Now we can learn the concepts for COCO-Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPT1ZBfTMvt9"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"task\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pACR1cFMvt9"
      },
      "source": [
        "Below you can find the concept learner snippets for the extension experiments which can be executed, if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAIPeNIZMvt9"
      },
      "outputs": [],
      "source": [
        "# Note: 'audio' here refers to AudioCLIP\n",
        "!python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"audioset+us8k+esc50\" \\\n",
        "  --backbone-name=\"audio\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2jtOZfkMvt9"
      },
      "source": [
        "# **Reproducing the Original Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u_lyIyEMvt9"
      },
      "source": [
        "We provide code to replicate the original results obtained.\n",
        "For the `datasets` parameter, we need to input a list of datasets we want to evaluate on.\n",
        "\n",
        "_Note:_ If you want, then all datasets can be evaluated by choosing the `eval-all` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9p5hhZIMvt9"
      },
      "outputs": [],
      "source": [
        "# Add \"PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1\" if you do not want to evaluate AudioCLIP\n",
        "!python eval_original_model.py \\\n",
        "  --datasets \"cifar10\" \"cifar100\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --num-workers=4 \\\n",
        "  --seeds 42 \\\n",
        "  --eval-all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv29xOw_Mvt9"
      },
      "source": [
        "# **Training PCBMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlJu6o6xMvt9"
      },
      "source": [
        "_Note:_ For the COCO-Stuff experiments, please set the `out-dir` to `artifacts/outdir/coco-stuff`or else the folder will be way less organized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43B0LGcaMvt-"
      },
      "source": [
        "## Main Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGITsMrDMvt-"
      },
      "source": [
        "The following cells are sample experiments for training the initial PCBMs. For other experiments, feel free to try the combinations specified by the authors (the base scripts for each experiment are present and should reproduce the results we obtain in our report. You can vary the parameters present).\n",
        "\n",
        "The `concept-bank` parameter should be the directory of the desired conceptbank in the following format below. This means that to get the CLIP concept results the concept bank should be changed accordingly. In addition, the `dataset-name` and `backbone-name` are the same as specified in the [table above](#training-and-evaluating-pcbms)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzP4MoEMMvt-"
      },
      "outputs": [],
      "source": [
        "#get the correct lambda values for each dataset\n",
        "cifar10_broden_lam = 2/(10 * 175)\n",
        "cifar100_broden_lam = 2/(100 * 175)\n",
        "cub_lam = 0.01/(200 * 112)\n",
        "ham_lam = 2.0/(2*8)\n",
        "isic_lam = 0.001/(2*8)\n",
        "coco_stuff_lam = 0.001\n",
        "\n",
        "cifar10_clip_lam = 2/(10 * 170)\n",
        "cifar100_clip_lam = 2/(100 * 440)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mF2M7q_Mvt-"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam={cifar10_broden_lam}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yRAg8dWMvt-"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"cifar100\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam={cifar100_broden_lam}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kdz-KwB7Mvt-"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"coco_stuff\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir/coco-stuff \\\n",
        "  --lam={coco_stuff_lam}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbY2v9TcMvt-"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/cub_resnet18_cub_10.0_50.pkl\" \\\n",
        "  --dataset=\"cub\" \\\n",
        "  --backbone-name=\"resnet18_cub\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam={cub_lam}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lezFBkhiMvt-"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/derm7pt_ham10000_inception_10.0_50.pkl\" \\\n",
        "  --dataset=\"ham10000\" \\\n",
        "  --backbone-name=\"ham10000_inception\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam={ham_lam}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ojvzvYMvt_"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/derm7pt_ham10000_inception_10.0_50.pkl\" \\\n",
        "  --dataset=\"siim_isic\" \\\n",
        "  --backbone-name=\"ham10000_inception\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam={isic_lam}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0fbc0MTMvt_"
      },
      "source": [
        "### Saliency Maps\n",
        "The following cells is the experiment of creating saliency maps for different concept. For Saliency maps from different classes change the 'targetclass' argument to a different class from the cifar100 dataset. Additionally the concepts names can be changed to different concepts from either the broden dataset or from 440 multimodal concepts for the CIFAR100 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENhI90jrMvt_"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python get_saliency_maps.py \\\n",
        "  --concept-bank1=\"artifacts/multimodal/mmc_clipRN50_cifar100_recurse_1.pkl\" \\\n",
        "  --concept-bank2=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"cifar100\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --concept-ix=0 \\\n",
        "  --method \"smoothgrad\" \\\n",
        "  --targetclass=\"bicycle\" \\\n",
        "  --concept-names1 'bicycle wheel' 'coaster brake' 'two wheels' 'bicycle seat' 'green' \\\n",
        "  --concept-names2 'bicycle' 'handle_bar' 'chain_wheel' 'book' 'greenness'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4r0Qn2zMvt_"
      },
      "source": [
        "### Model Editing Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yad3MMW9Mvt_"
      },
      "source": [
        "By running the following script, you will perform the model editing experiments for the 6 scenarios and one seed. Feel free to add seeds or change the base model by swapping the base_config. The results will be found in a .csv file at `logs/base_clip_resnet50/0/{timestamp}` (or more generally at `logs/{model_name}/{seed0}-{...}-{seedn}/{timestamp}`). Make sure Broden CAV concepts with C=0.01 are dowloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct9-nyuRMvt_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress output with capture magic\n",
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python -m experiments.model_editing.make_table_sk \\\n",
        "    --seed 0 \\\n",
        "    --base_config configs/model_editing/classifier/sk_base_clip_resnet50.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh8KNJaXMvt_"
      },
      "source": [
        "### Audio Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cA6GeueMvt_"
      },
      "outputs": [],
      "source": [
        "!python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/multimodal/mmc_audio_us8k_recurse_1.pkl\" \\\n",
        "  --dataset=\"us8k\" \\\n",
        "  --backbone-name=\"audio\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam=2e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqevPP2VMvt_"
      },
      "source": [
        "### Concept Feature Value Experiment\n",
        "For this experiment we also need to get the multimodal conceptbank for clip as we do in the second cell below. To get the tree different results in the table vary the Concept bank from CAVs to Multimodal and use the --random_proj parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T4C5D4sMvuA"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"broden\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DAiuS9lMvuA"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python test_cav_activation.py \\\n",
        "    --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "    --concept-dataset=\"broden\" \\\n",
        "    --backbone-name=\"clip:RN50\" \\\n",
        "    --out-dir=\"artifacts\" \\\n",
        "    --alpha=0.99 \\\n",
        "    --num-workers=2 \\\n",
        "    --seeds '42'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzL2aXVYMvuA"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"cub\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOmfeJcYMvuA"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python test_cav_activation.py \\\n",
        "    --concept-bank=\"artifacts/multimodal/mmc_clipRN50_cub_recurse_1.pkl\" \\\n",
        "    --concept-dataset=\"cub\" \\\n",
        "    --dataset=\"cub\" \\\n",
        "    --backbone-name=\"clip:RN50\" \\\n",
        "    --out-dir=\"artifacts\" \\\n",
        "    --alpha=0.99 \\\n",
        "    --num-workers=2 \\\n",
        "    --seeds '42'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjmbSTxMvuA"
      },
      "source": [
        "### Random Projection Experiment\n",
        "The code below is the template for the reuslts of one dataset. To get the results for the other datasets in the report change the parameters to those seen in the main experiments. Make sure the '--random_proj' and '--seeds' parameters are kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGfd7_9pMvuA"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python verify_dataset_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir='artifacts' \\\n",
        "  --lam={cifar10_lam} \\\n",
        "  --seeds '42' \\\n",
        "  --random_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W42dPSDUMvuA"
      },
      "source": [
        "### Complexity-Acurracy trade-off results and plots\n",
        "The results will be in the sum.png and sparsities.png of the artifacts folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuD3Ra8yMvuA"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=models:.:$$PYTHONPATH NO_AUDIOCLIP=1 python /content/Anonymous/test_sparsity_vs_accuracy.py \\\n",
        "    --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "    --dataset=\"cifar100\" \\\n",
        "    --backbone-name=\"clip:RN50\" \\\n",
        "    --out-dir=\"artifacts\" \\\n",
        "    --alpha=0.99 \\\n",
        "    --strengths 10.0 1.0 0.1 0.01 0.001 \\\n",
        "    --num-workers=2 \\\n",
        "    --seed 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK2m6J2jMvuB"
      },
      "source": [
        "# **Training PCBM-h's**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc2yMtf9MvuB"
      },
      "source": [
        "## Main Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htr6Xq3GMvuB"
      },
      "source": [
        "For this section, make sure to input the path to where the desired PCBM was saved for `pcbm-path`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4_jmNIuMvuB"
      },
      "outputs": [],
      "source": [
        "# You can adjust the 'seed' part in the pcbm-path variable\n",
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/pcbm_cifar10__clipRN50__broden_clipRN50_10__lam_0.0002__alpha_0.99__seed_42.ckpt\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --num-workers=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhen2ZfcMvuB"
      },
      "outputs": [],
      "source": [
        "# You can adjust the 'seed' part in the pcbm-path variable\n",
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/pcbm_cifar100__clipRN50__broden_clipRN50_10__lam_0.0002__alpha_0.99__seed_42.ckpt\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"cifar100\" \\\n",
        "  --num-workers=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ1Wl36FMvuB"
      },
      "outputs": [],
      "source": [
        "# You can adjust the 'seed' part in the pcbm-path variable\n",
        "!PYTHONPATH=models:.:$PYTHONPATH NO_AUDIOCLIP=1 python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/coco-stuff/pcbm_coco_stuff__clipRN50__broden_clipRN50_10__lam_0.0002__alpha_0.99__seed_42_target_3.ckpt\" \\\n",
        "  --out-dir=artifacts/outdir/coco-stuff \\\n",
        "  --dataset=\"coco_stuff\" \\\n",
        "  --num-workers=4 \\\n",
        "  --no-print-out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tzQUgfGMvuB"
      },
      "source": [
        "## Extension Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOnXbQzfMvuB"
      },
      "outputs": [],
      "source": [
        "!python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/multimodal/mmc_audio_audioset+us8k+esc50_recurse_1.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"us8k\" \\\n",
        "  --num-workers=4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}